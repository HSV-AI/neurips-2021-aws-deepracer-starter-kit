method: random
metric:
  name: eval/mean_reward
  goal: maximize
parameters:
  n_envs: 
    value: 4
  eval_episodes:
    value: 1000
  batch_size:
    values:
    - 4
    - 8
    - 16
    - 32
    - 64
    - 128
    - 256
    - 512
    - 1024
  clip_range_vf:
    values:
    - None
    - 0.0
    - 0.05
    - 0.1
    - 0.2
    - 0.3
    - 0.4
    - 0.5
    - 1.0
  clip_range:
    values:
    - 0.00
    - 0.05
    - 0.1
    - 0.2
    - 0.3
    - 0.4
    - 0.5
    - 1.0
  ent_coef:
    values:
    - 0.0
    - 0.1
    - 0.01
    - 0.001
    - 0.0001
    - 0.00001
  gae_lambda:
    values:
    - 1.0
    - 0.95
    - 0.9
  gamma:
    values:
    - 0.999
    - 0.995
    - 0.99
    - 0.95
    - 0.9
    - 0.8
    - 0.5
  learning_rate:
    values:
    - 0.000001
    - 0.00001
    - 0.00005
    - 0.0001
    - 0.0005
    - 0.001
    - 0.005
    - 0.01
    - 0.05
  max_grad_norm:
    values:
    - 0.1
    - 0.2
    - 0.5
    - 1.0
  n_epochs:
    values:
    - 1
    - 2
    - 4
    - 8
    - 16
    - 32
  rollout:
    values:
    - 1024
    - 2048
    - 4096
    - 8192
    - 16384
    - 32768
    - 65536
  stack_size:
    values:
    - 1
    - 2
    - 4
    - 8
    - 16
    - 32
    - 64
  steps:
    values:
      - 10000
      - 50000
      - 100000
      - 500000
      - 1000000
      - 5000000
      - 10000000
      - 50000000
  vf_coef:
    values:
    - 1.0
    - 0.8
    - 0.5
    - 0.1
  normalize:
    values:
    - True
    - False
program: baseline.py